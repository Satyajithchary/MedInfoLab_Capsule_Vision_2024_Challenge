{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9710554,"sourceType":"datasetVersion","datasetId":5939611},{"sourceId":9711362,"sourceType":"datasetVersion","datasetId":5940241},{"sourceId":9711732,"sourceType":"datasetVersion","datasetId":5940524},{"sourceId":9712856,"sourceType":"datasetVersion","datasetId":5941418}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport os\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nimport torch.nn as nn\nimport torch.optim as optim\n!pip install open_clip_torch==2.23.0 transformers==4.35.2 matplotlib\nfrom open_clip import create_model_and_transforms, get_tokenizer\nimport pandas as pd\nimport numpy as np\n\n# Custom Dataset to handle nested subfolders\nclass CustomImageDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.image_paths = []\n        self.labels = []\n        self.class_to_idx = self.get_class_to_idx()\n\n        # Populate image paths and corresponding labels\n        self.load_images()\n\n    def get_class_to_idx(self):\n        \"\"\"Map class names to labels based on the top-level directories (class folders).\"\"\"\n        classes = sorted([d.name for d in os.scandir(self.root_dir) if d.is_dir()])\n        return {cls_name: idx for idx, cls_name in enumerate(classes)}\n\n    def load_images(self):\n        \"\"\"Recursively find all images in the nested folders and associate them with labels.\"\"\"\n        for class_name, class_idx in self.class_to_idx.items():\n            class_folder = os.path.join(self.root_dir, class_name)\n            for root, _, files in os.walk(class_folder):\n                for file in files:\n                    if file.endswith(('.png', '.jpg', '.jpeg')):\n                        file_path = os.path.join(root, file)\n                        self.image_paths.append(file_path)\n                        self.labels.append(class_idx)\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image_path = self.image_paths[idx]\n        label = self.labels[idx]\n\n        # Load image\n        image = Image.open(image_path).convert('RGB')\n\n        # Apply transformations if provided\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\n# Define the transform to resize and normalize images\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n])\n\n# Load the training and validation datasets using the custom dataset\ntrain_dataset = CustomImageDataset(root_dir='/kaggle/input/dataset01/Dataset/training', transform=transform)\nval_dataset = CustomImageDataset(root_dir='/kaggle/input/dataset01/Dataset/validation', transform=transform)\n\n# Create DataLoader for training and validation\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n\n# Display class mapping (class names to index)\nprint(train_dataset.class_to_idx)\n\n# Load the BiomedCLIP model and tokenizer from Hugging Face hub\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel, preprocess, _ = create_model_and_transforms('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n# Move the model to the device\nmodel.to(device)\n\n# Load the tokenizer for PubMedBERT\ntokenizer = get_tokenizer('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n\n# Manually define the class names\nclass_names = ['Angioectasia', 'Bleeding', 'Erosion', 'Erythema', 'Foreign Body', \n               'Lymphangiectasia', 'Normal', 'Polyp', 'Ulcer', 'Worms']\n\n# Function to extract text embeddings using PubMedBERT for class names\ndef get_text_embeddings(class_names, template='This is an endoscopic photo of '):\n    # Tokenize the class names by concatenating them with the template\n    tokenized_inputs = tokenizer([template + name for name in class_names], context_length=256).to(device)\n    \n    # Generate text embeddings using the model\n    with torch.no_grad():\n        text_embeddings = model.encode_text(tokenized_inputs)\n    \n    return text_embeddings\n\n# Extract embeddings for the class names\nclass_name_embeddings = get_text_embeddings(class_names)\n\n# Define loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n\n# Training loop\ndef train_multimodal(model, train_loader, class_name_embeddings, optimizer, criterion, device):\n    model.train()\n    total_loss = 0\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        \n        # Forward pass for images\n        image_features = model.encode_image(images)\n        \n        # Forward pass for text embeddings (class names)\n        text_features = class_name_embeddings[labels]\n        \n        # Compute cosine similarity between image and text features\n        logits = torch.matmul(image_features, text_features.T)\n        \n        # Loss computation\n        loss = criterion(logits, labels)\n        \n        # Backward pass and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n    \n    return total_loss / len(train_loader)\n\n\n\n# Validation Loop with Prediction Saving\ndef validate_and_save_predictions(model, val_loader, class_name_embeddings, device, output_path='predictions.xlsx'):\n    model.eval()\n    predictions = []\n    image_paths = []\n    \n    with torch.no_grad():\n        for images, labels in val_loader:\n            images = images.to(device)\n            \n            # Get image embeddings\n            image_features = model.encode_image(images)\n            \n            # Compute similarity with class name embeddings\n            logits = torch.matmul(image_features, class_name_embeddings.T)\n            probabilities = torch.softmax(logits, dim=1).cpu().numpy()\n            \n            # Store predicted probabilities\n            predictions.extend(probabilities)\n            \n            # Access image paths directly from the dataset\n            batch_image_paths = [val_loader.dataset.image_paths[idx] for idx in range(len(labels))]\n            image_paths.extend(batch_image_paths)\n    \n    # Save predictions to Excel\n    save_predictions_to_excel(image_paths, np.array(predictions), output_path)\n\n# Function to save predictions to Excel\ndef save_predictions_to_excel(image_paths, y_pred, output_path):\n    classes = ['Angioectasia', 'Bleeding', 'Erosion', 'Erythema', 'Foreign Body', \n               'Lymphangiectasia', 'Normal', 'Polyp', 'Ulcer', 'Worms']\n    \n    data = {'image_path': image_paths}\n    for i, class_name in enumerate(classes):\n        data[class_name] = y_pred[:, i]\n    \n    predicted_classes = np.argmax(y_pred, axis=1)\n    data['predicted_class'] = [classes[i] for i in predicted_classes]\n    \n    df = pd.DataFrame(data)\n    df.to_excel(output_path, index=False)\n\n# Training and Validation\nnum_epochs = 1\nfor epoch in range(num_epochs):\n    train_loss = train_multimodal(model, train_loader, class_name_embeddings, optimizer, criterion, device)\n\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}')\n    \n    # Validate and save predictions at the end of each epoch\n    if epoch == num_epochs - 1:\n        validate_and_save_predictions(model, val_loader, class_name_embeddings, device, output_path='validation_predictions.xlsx')\n\n# Save the trained model\ntorch.save(model, 'best_model.pth')\n# Testing on new images\ndef test_model_on_images(model, test_image_dir, class_name_embeddings, output_excel_path, preprocess, device):\n    image_paths = []\n    \n    # Traverse test image directory and collect all image paths\n    for root, _, files in os.walk(test_image_dir):\n        for file in files:\n            if file.endswith(('.png', '.jpg', '.jpeg')):\n                image_paths.append(os.path.join(root, file))\n    \n    results = []\n    \n    # Class names for reference\n    classes = ['Angioectasia', 'Bleeding', 'Erosion', 'Erythema', 'Foreign Body', \n               'Lymphangiectasia', 'Normal', 'Polyp', 'Ulcer', 'Worms']\n    \n    # Looping over test images\n    for img_path in image_paths:\n        image = Image.open(img_path).convert('RGB')\n        image_tensor = preprocess(image).unsqueeze(0).to(device)\n        \n        with torch.no_grad():\n            # Encode the image\n            image_features = model.encode_image(image_tensor)\n            \n            # Compute similarity between image features and class name embeddings\n            logits = torch.matmul(image_features, class_name_embeddings.T)\n            probabilities = torch.softmax(logits, dim=1).cpu().numpy()\n            \n            # Get the most probable class\n            predicted_class_idx = probabilities.argmax(axis=1).item()\n            predicted_class = classes[predicted_class_idx]\n            \n            # Store image path, all class probabilities, and predicted class\n            result = {'image_path': os.path.basename(img_path)} \n            for i, class_name in enumerate(classes):\n                result[class_name] = probabilities[0, i]  # Add the probability for each class\n            result['predicted_class'] = predicted_class  # Add predicted class\n            \n            results.append(result)\n    \n    # Convert the results to a DataFrame and save to Excel\n    results_df = pd.DataFrame(results)\n    results_df.to_excel(output_excel_path, index=False)\n\n# Example of testing the model on new images\ntest_model_on_images(model, '/kaggle/input/cvc-test-2024/Testing set/Images', class_name_embeddings, 'test_predictions.xlsx', preprocess, device)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T13:06:58.007447Z","iopub.execute_input":"2024-10-25T13:06:58.007760Z","iopub.status.idle":"2024-10-25T13:30:45.884065Z","shell.execute_reply.started":"2024-10-25T13:06:58.007723Z","shell.execute_reply":"2024-10-25T13:30:45.882999Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting open_clip_torch==2.23.0\n  Downloading open_clip_torch-2.23.0-py3-none-any.whl.metadata (30 kB)\nCollecting transformers==4.35.2\n  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (3.7.5)\nRequirement already satisfied: torch>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from open_clip_torch==2.23.0) (2.4.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from open_clip_torch==2.23.0) (0.19.0)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from open_clip_torch==2.23.0) (2024.5.15)\nCollecting ftfy (from open_clip_torch==2.23.0)\n  Downloading ftfy-6.3.0-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from open_clip_torch==2.23.0) (4.66.4)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from open_clip_torch==2.23.0) (0.25.1)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from open_clip_torch==2.23.0) (0.2.0)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from open_clip_torch==2.23.0) (3.20.3)\nRequirement already satisfied: timm in /opt/conda/lib/python3.10/site-packages (from open_clip_torch==2.23.0) (1.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2) (2.32.3)\nCollecting tokenizers<0.19,>=0.14 (from transformers==4.35.2)\n  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2) (0.4.5)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (10.3.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->open_clip_torch==2.23.0) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->open_clip_torch==2.23.0) (4.12.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch==2.23.0) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch==2.23.0) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch==2.23.0) (3.1.4)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from ftfy->open_clip_torch==2.23.0) (0.2.13)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.35.2) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.35.2) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.35.2) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.35.2) (2024.8.30)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.9.0->open_clip_torch==2.23.0) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.9.0->open_clip_torch==2.23.0) (1.3.0)\nDownloading open_clip_torch-2.23.0-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading ftfy-6.3.0-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: ftfy, tokenizers, transformers, open_clip_torch\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.20.0\n    Uninstalling tokenizers-0.20.0:\n      Successfully uninstalled tokenizers-0.20.0\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.45.1\n    Uninstalling transformers-4.45.1:\n      Successfully uninstalled transformers-4.45.1\nSuccessfully installed ftfy-6.3.0 open_clip_torch-2.23.0 tokenizers-0.15.2 transformers-4.35.2\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n  _torch_pytree._register_pytree_node(\n/opt/conda/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n  _torch_pytree._register_pytree_node(\n","output_type":"stream"},{"name":"stdout","text":"{'Angioectasia': 0, 'Bleeding': 1, 'Erosion': 2, 'Erythema': 3, 'Foreign Body': 4, 'Lymphangiectasia': 5, 'Normal': 6, 'Polyp': 7, 'Ulcer': 8, 'Worms': 9}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"open_clip_pytorch_model.bin:   0%|          | 0.00/784M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98956ffdedf34c22851524a880f7439d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"open_clip_config.json:   0%|          | 0.00/707 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e1a9b66f3734fb1867d4ed2c3e588c1"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40e366093a124d3785e6591997ddc879"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n  _torch_pytree._register_pytree_node(\n/opt/conda/lib/python3.10/site-packages/open_clip/factory.py:129: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(checkpoint_path, map_location=map_location)\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"386c0ff162894d12b25dab38ed2f067d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/225k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf1a2a985f6e475e8fd6d314a4aae37e"}},"metadata":{}},{"name":"stdout","text":"Epoch [1/1], Loss: 6.0864\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Graphs**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Function to plot accuracy\ndef plot_accuracy(train_accuracies, val_accuracies, num_epochs):\n    epochs = range(1, num_epochs + 1)\n    \n    plt.figure(figsize=(10, 6))\n    plt.plot(epochs, train_accuracies, 'b', label='Training Accuracy')\n    plt.plot(epochs, val_accuracies, 'r', label='Validation Accuracy')\n    plt.title('Training and Validation Accuracy over Epochs')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.show()\n\n# Example usage: After training, pass in train and val accuracies collected during training\n# plot_accuracy(train_accuracies, val_accuracies, num_epochs)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T13:58:43.577351Z","iopub.execute_input":"2024-10-25T13:58:43.578118Z","iopub.status.idle":"2024-10-25T13:58:43.584340Z","shell.execute_reply.started":"2024-10-25T13:58:43.578076Z","shell.execute_reply":"2024-10-25T13:58:43.583477Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Function to plot loss\ndef plot_loss(train_losses, val_losses, num_epochs):\n    epochs = range(1, num_epochs + 1)\n    \n    plt.figure(figsize=(10, 6))\n    plt.plot(epochs, train_losses, 'b', label='Training Loss')\n    plt.plot(epochs, val_losses, 'r', label='Validation Loss')\n    plt.title('Training and Validation Loss over Epochs')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()\n\n# Example usage: After training, pass in train and val losses collected during training\n# plot_loss(train_losses, val_losses, num_epochs)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T13:58:43.835422Z","iopub.execute_input":"2024-10-25T13:58:43.835977Z","iopub.status.idle":"2024-10-25T13:58:43.842939Z","shell.execute_reply.started":"2024-10-25T13:58:43.835939Z","shell.execute_reply":"2024-10-25T13:58:43.841636Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\n# Function to plot confusion matrix\ndef plot_confusion_matrix(true_labels, pred_labels, class_names):\n    cm = confusion_matrix(true_labels, pred_labels)\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted Labels')\n    plt.ylabel('True Labels')\n    plt.show()\n\n# Example usage: After predictions, pass in true and predicted labels along with class names\n# plot_confusion_matrix(true_labels, pred_labels, class_names)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T13:58:44.157574Z","iopub.execute_input":"2024-10-25T13:58:44.157985Z","iopub.status.idle":"2024-10-25T13:58:44.622547Z","shell.execute_reply.started":"2024-10-25T13:58:44.157947Z","shell.execute_reply":"2024-10-25T13:58:44.621535Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\n\n# Function to plot ROC curve for each class\ndef plot_roc_curve(true_labels, pred_probs, class_names):\n    # Binarize the true labels for multi-class ROC\n    true_labels_bin = label_binarize(true_labels, classes=range(len(class_names)))\n    n_classes = len(class_names)\n    \n    plt.figure(figsize=(10, 8))\n    \n    # Plot ROC curve for each class\n    for i in range(n_classes):\n        fpr, tpr, _ = roc_curve(true_labels_bin[:, i], pred_probs[:, i])\n        roc_auc = auc(fpr, tpr)\n        plt.plot(fpr, tpr, label=f'{class_names[i]} (AUC = {roc_auc:.2f})')\n    \n    plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curves')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n# Example usage: After predictions, pass in true labels and predicted probabilities\n# plot_roc_curve(true_labels, pred_probs, class_names)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T13:58:44.714056Z","iopub.execute_input":"2024-10-25T13:58:44.714573Z","iopub.status.idle":"2024-10-25T13:58:44.723420Z","shell.execute_reply.started":"2024-10-25T13:58:44.714530Z","shell.execute_reply":"2024-10-25T13:58:44.722423Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\n\n# Function to plot Precision-Recall curve for each class\ndef plot_precision_recall_curve(true_labels, pred_probs, class_names):\n    # Binarize the true labels for multi-class PR curve\n    true_labels_bin = label_binarize(true_labels, classes=range(len(class_names)))\n    n_classes = len(class_names)\n    \n    plt.figure(figsize=(10, 8))\n    \n    # Plot Precision-Recall curve for each class\n    for i in range(n_classes):\n        precision, recall, _ = precision_recall_curve(true_labels_bin[:, i], pred_probs[:, i])\n        plt.plot(recall, precision, label=f'{class_names[i]}')\n    \n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision-Recall Curves')\n    plt.legend(loc=\"lower left\")\n    plt.show()\n\n# Example usage: After predictions, pass in true labels and predicted probabilities\n# plot_precision_recall_curve(true_labels, pred_probs, class_names)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T13:58:45.199728Z","iopub.execute_input":"2024-10-25T13:58:45.200816Z","iopub.status.idle":"2024-10-25T13:58:45.208692Z","shell.execute_reply.started":"2024-10-25T13:58:45.200769Z","shell.execute_reply":"2024-10-25T13:58:45.207589Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_accuracies = []\nval_accuracies = []\ntrain_losses = []\nval_losses = []\n\nfor epoch in range(num_epochs):\n    train_loss, train_acc = train_multimodal(model, train_loader, class_name_embeddings, optimizer, criterion, device)  # Your train function\n    val_loss, val_acc = validate_multimodal(model, val_loader, class_name_embeddings, optimizer, criterion, device)  # Your validation function\n    \n    train_losses.append(train_loss)\n    val_losses.append(val_loss)\n    train_accuracies.append(train_acc)\n    val_accuracies.append(val_acc)\n\n# After training, generate the graphs\nplot_accuracy(train_accuracies, val_accuracies, num_epochs)\nplot_loss(train_losses, val_losses, num_epochs)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T13:58:46.465653Z","iopub.execute_input":"2024-10-25T13:58:46.466006Z","iopub.status.idle":"2024-10-25T14:11:58.932852Z","shell.execute_reply.started":"2024-10-25T13:58:46.465974Z","shell.execute_reply":"2024-10-25T14:11:58.931461Z"},"trusted":true},"execution_count":10,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m val_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m----> 7\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m train_multimodal(model, train_loader, class_name_embeddings, optimizer, criterion, device)  \u001b[38;5;66;03m# Your train function\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m validate_multimodal(model, val_loader, class_name_embeddings, optimizer, criterion, device)  \u001b[38;5;66;03m# Your validation function\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n","\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable float object"],"ename":"TypeError","evalue":"cannot unpack non-iterable float object","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import roc_auc_score, precision_recall_fscore_support, confusion_matrix, balanced_accuracy_score\nfrom sklearn.preprocessing import label_binarize\nimport json\n\ndef generate_metrics_report(y_true, y_pred):\n    # Class names for reference\n    class_names = ['Angioectasia', 'Bleeding', 'Erosion', 'Erythema', 'Foreign Body', \n                   'Lymphangiectasia', 'Normal', 'Polyp', 'Ulcer', 'Worms']\n    \n    # Initialize metrics dictionary\n    metrics_report = {}\n    \n    # Ensure y_true and y_pred are numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Convert one-hot encoded y_true to class labels\n    y_true_labels = np.argmax(y_true, axis=1)\n    y_pred_labels = np.argmax(y_pred, axis=1)\n    \n    # Confusion matrix\n    cm = confusion_matrix(y_true_labels, y_pred_labels)\n    \n    # Specificity (True Negative Rate)\n    specificity = []\n    for i in range(len(class_names)):\n        tn = cm.sum() - (cm[i, :].sum() + cm[:, i].sum() - cm[i, i])\n        fp = cm[:, i].sum() - cm[i, i]\n        specificity.append(tn / (tn + fp) if (tn + fp) > 0 else 0)\n    \n    # Sensitivity (Recall), Precision, F1 Score, and Support\n    precision, recall, f1, _ = precision_recall_fscore_support(y_true_labels, y_pred_labels, average=None, labels=range(len(class_names)))\n    \n    # ROC AUC score for each class (One-vs-Rest)\n    roc_auc = []\n    for i in range(len(class_names)):\n        roc_auc.append(roc_auc_score(y_true[:, i], y_pred[:, i]))\n    \n    # Mean AUC across all classes\n    mean_auc = np.mean(roc_auc)\n    \n    # Balanced Accuracy Score\n    balanced_acc = balanced_accuracy_score(y_true_labels, y_pred_labels)\n    \n    # Aggregated Metrics\n    metrics_report['mean_auc'] = mean_auc\n    metrics_report['balanced_accuracy'] = balanced_acc\n    \n    # Class-wise Metrics\n    class_metrics = {}\n    for i, class_name in enumerate(class_names):\n        class_metrics[class_name] = {\n            'specificity': specificity[i],\n            'roc_auc': roc_auc[i],\n            'precision': precision[i],\n            'recall': recall[i],\n            'f1_score': f1[i]\n        }\n    \n    metrics_report['class_metrics'] = class_metrics\n    \n    # Convert to JSON string\n    return json.dumps(metrics_report, indent=4)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T21:12:43.626229Z","iopub.execute_input":"2024-10-24T21:12:43.626891Z","iopub.status.idle":"2024-10-24T21:12:43.640393Z","shell.execute_reply.started":"2024-10-24T21:12:43.626851Z","shell.execute_reply":"2024-10-24T21:12:43.639339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}