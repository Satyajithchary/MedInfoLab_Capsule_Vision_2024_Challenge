{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install opencv-python","metadata":{},"execution_count":1,"outputs":[{"name":"stdout","output_type":"stream","text":"Requirement already satisfied: opencv-python in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (4.10.0.84)\n\nRequirement already satisfied: numpy>=1.17.3; python_version >= \"3.8\" in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from opencv-python) (1.24.4)\n\nNote: you may need to restart the kernel to use updated packages.\n"}]},{"cell_type":"code","source":"pip install open_clip_torch==2.23.0 transformers==4.35.2 matplotlib","metadata":{},"execution_count":2,"outputs":[{"name":"stdout","output_type":"stream","text":"Requirement already satisfied: open_clip_torch==2.23.0 in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (2.23.0)\n\nRequirement already satisfied: transformers==4.35.2 in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (4.35.2)\n\nRequirement already satisfied: matplotlib in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (3.7.5)\n\nRequirement already satisfied: regex in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from open_clip_torch==2.23.0) (2024.9.11)\n\nRequirement already satisfied: tqdm in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from open_clip_torch==2.23.0) (4.66.5)\n\nRequirement already satisfied: torch>=1.9.0 in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from open_clip_torch==2.23.0) (2.4.1)\n\nRequirement already satisfied: protobuf in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from open_clip_torch==2.23.0) (5.28.3)\n\nRequirement already satisfied: timm in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from open_clip_torch==2.23.0) (1.0.11)\n\nRequirement already satisfied: ftfy in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from open_clip_torch==2.23.0) (6.2.3)\n\nRequirement already satisfied: sentencepiece in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from open_clip_torch==2.23.0) (0.2.0)\n\nRequirement already satisfied: torchvision in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from open_clip_torch==2.23.0) (0.19.1)\n\nRequirement already satisfied: huggingface-hub in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from open_clip_torch==2.23.0) (0.26.1)\n\nRequirement already satisfied: filelock in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from transformers==4.35.2) (3.16.1)\n\nRequirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers==4.35.2) (2.22.0)\n\nRequirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from transformers==4.35.2) (20.3)\n\nRequirement already satisfied: numpy>=1.17 in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from transformers==4.35.2) (1.24.4)\n\nRequirement already satisfied: safetensors>=0.3.1 in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from transformers==4.35.2) (0.4.5)\n\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from transformers==4.35.2) (0.15.2)\n\nRequirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers==4.35.2) (5.3.1)\n\nRequirement already satisfied: kiwisolver>=1.0.1 in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from matplotlib) (1.4.7)\n\nRequirement already satisfied: contourpy>=1.0.1 in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from matplotlib) (1.1.1)\n\nRequirement already satisfied: python-dateutil>=2.7 in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from matplotlib) (2.9.0.post0)\n\nRequirement already satisfied: cycler>=0.10 in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from matplotlib) (0.12.1)\n\nRequirement already satisfied: importlib-resources>=3.2.0; python_version < \"3.10\" in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from matplotlib) (6.4.5)\n\nRequirement already satisfied: pillow>=6.2.0 in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from matplotlib) (10.4.0)\n\nRequirement already satisfied: fonttools>=4.22.0 in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from matplotlib) (4.54.1)\n\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.6)\n\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from torch>=1.9.0->open_clip_torch==2.23.0) (10.3.2.106)\n\nRequirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch>=1.9.0->open_clip_torch==2.23.0) (2.10.1)\n\nRequirement already satisfied: fsspec in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from torch>=1.9.0->open_clip_torch==2.23.0) (2024.10.0)\n\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from torch>=1.9.0->open_clip_torch==2.23.0) (11.4.5.107)\n\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from torch>=1.9.0->open_clip_torch==2.23.0) (12.1.105)\n\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from torch>=1.9.0->open_clip_torch==2.23.0) (12.1.105)\n\nRequirement already satisfied: sympy in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from torch>=1.9.0->open_clip_torch==2.23.0) (1.13.3)\n\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from torch>=1.9.0->open_clip_torch==2.23.0) (11.0.2.54)\n\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from torch>=1.9.0->open_clip_torch==2.23.0) (12.1.105)\n\nRequirement already satisfied: triton==3.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version < \"3.13\" in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from torch>=1.9.0->open_clip_torch==2.23.0) (3.0.0)\n\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from torch>=1.9.0->open_clip_torch==2.23.0) (12.1.0.106)\n\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from torch>=1.9.0->open_clip_torch==2.23.0) (2.20.5)\n\nRequirement already satisfied: networkx in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from torch>=1.9.0->open_clip_torch==2.23.0) (3.1)\n\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from torch>=1.9.0->open_clip_torch==2.23.0) (12.1.105)\n\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from torch>=1.9.0->open_clip_torch==2.23.0) (12.1.3.1)\n\nRequirement already satisfied: typing-extensions>=4.8.0 in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from torch>=1.9.0->open_clip_torch==2.23.0) (4.12.2)\n\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from torch>=1.9.0->open_clip_torch==2.23.0) (9.1.0.70)\n\nRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from ftfy->open_clip_torch==2.23.0) (0.2.13)\n\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.14.0)\n\nRequirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from importlib-resources>=3.2.0; python_version < \"3.10\"->matplotlib) (3.20.2)\n\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch>=1.9.0->open_clip_torch==2.23.0) (12.6.77)\n\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from sympy->torch>=1.9.0->open_clip_torch==2.23.0) (1.3.0)\n\nNote: you may need to restart the kernel to use updated packages.\n"}]},{"cell_type":"code","source":"import cv2 ","metadata":{},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"pip install torch transformers tqdm","metadata":{},"execution_count":6,"outputs":[{"name":"stdout","output_type":"stream","text":"Requirement already satisfied: torch in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (2.4.1)\n\nRequirement already satisfied: transformers in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (4.35.2)\n\nRequirement already satisfied: tqdm in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (4.66.5)\n\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from torch) (12.1.0.106)\n\nRequirement already satisfied: triton==3.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version < \"3.13\" in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from torch) (3.0.0)\n\nRequirement already satisfied: filelock in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from torch) (3.16.1)\n\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from torch) (12.1.105)\n\nRequirement already satisfied: sympy in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from torch) (1.13.3)\n\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from torch) (12.1.105)\n\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from torch) (12.1.105)\n\nRequirement already satisfied: fsspec in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from torch) (2024.10.0)\n\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from torch) (12.1.3.1)\n\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from torch) (10.3.2.106)\n\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from torch) (2.20.5)\n\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from torch) (11.0.2.54)\n\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from torch) (11.4.5.107)\n\nRequirement already satisfied: networkx in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from torch) (3.1)\n\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from torch) (9.1.0.70)\n\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from torch) (12.1.105)\n\nRequirement already satisfied: typing-extensions>=4.8.0 in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from torch) (4.12.2)\n\nRequirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch) (2.10.1)\n\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from transformers) (0.26.1)\n\nRequirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers) (2.22.0)\n\nRequirement already satisfied: safetensors>=0.3.1 in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from transformers) (0.4.5)\n\nRequirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.3.1)\n\nRequirement already satisfied: regex!=2019.12.17 in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from transformers) (2024.9.11)\n\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from transformers) (0.15.2)\n\nRequirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from transformers) (20.3)\n\nRequirement already satisfied: numpy>=1.17 in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from transformers) (1.24.4)\n\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch) (12.6.77)\n\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /raid/ai23mtech14007/.local/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n\nNote: you may need to restart the kernel to use updated packages.\n"}]},{"cell_type":"markdown","source":"# Final Code","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\nimport open_clip\nimport pandas as pd\n\nclass BiomedCLIPClassifier(nn.Module):\n    def __init__(self, model_name, num_classes):\n        super().__init__()\n        self.model, self.preprocess, _ = open_clip.create_model_and_transforms(\n            'hf-hub:' + model_name\n        )\n        self.tokenizer = open_clip.get_tokenizer('hf-hub:' + model_name)\n        self.logit_scale = nn.Parameter(torch.ones([]) * torch.log(torch.tensor(1/0.07)))\n        \n        self.class_names = ['Angioectasia', 'Bleeding', 'Erosion', 'Erythema', \n                           'Foreign Body', 'Lymphangiectasia', 'Normal', 'Polyp', \n                           'Ulcer', 'Worms']\n        self.register_buffer('text_features', self._encode_text(self.class_names))\n        \n    def _encode_text(self, class_names):\n        text_tokens = self.tokenizer(class_names)\n        with torch.no_grad():\n            text_features = self.model.encode_text(text_tokens)\n            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n        return text_features\n    \n    def forward(self, images):\n        image_features = self.model.encode_image(images)\n        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n        logits = torch.matmul(image_features, self.text_features.t()) * self.logit_scale.exp()\n        return logits\n\nclass CustomImageDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.class_to_idx = {\n            'Angioectasia': 0, 'Bleeding': 1, 'Erosion': 2, 'Erythema': 3,\n            'Foreign Body': 4, 'Lymphangiectasia': 5, 'Normal': 6, 'Polyp': 7,\n            'Ulcer': 8, 'Worms': 9\n        }\n        \n        self.samples = []\n        for class_name in os.listdir(root_dir):\n            class_dir = os.path.join(root_dir, class_name)\n            if os.path.isdir(class_dir):\n                for subfolder in os.listdir(class_dir):\n                    subfolder_path = os.path.join(class_dir, subfolder)\n                    if os.path.isdir(subfolder_path):\n                        for img_name in os.listdir(subfolder_path):\n                            if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n                                img_path = os.path.join(subfolder_path, img_name)\n                                self.samples.append((img_path, self.class_to_idx[class_name], subfolder))  # Save subfolder as Dataset name\n    \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        img_path, label, dataset_name = self.samples[idx]\n        image = Image.open(img_path).convert('RGB')\n        \n        if self.transform:\n            image = self.transform(image)\n            \n        return image, label, img_path, dataset_name  # Return dataset_name as well\n\nclass TestDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.images = [\n            os.path.join(root_dir, img_name)\n            for img_name in os.listdir(root_dir)\n            if img_name.lower().endswith(('.jpg', '.jpeg', '.png'))\n        ]\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_path = self.images[idx]\n        image = Image.open(img_path).convert('RGB')\n\n        if self.transform:\n            image = self.transform(image)\n\n        # Only return the image and the filename (basename of path)\n        return image, os.path.basename(img_path)\n    \n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\nimport open_clip\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import (\n    balanced_accuracy_score, roc_auc_score, roc_curve,\n    precision_recall_curve, average_precision_score,\n    f1_score, confusion_matrix\n)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom itertools import cycle\n\n# [Previous model and dataset classes remain the same...]\n# BiomedCLIPClassifier, CustomImageDataset, and TestDataset classes stay unchanged\n\nclass MetricsTracker:\n    def __init__(self):\n        self.train_losses = []\n        self.train_accs = []\n        self.val_losses = []\n        self.val_accs = []\n        self.epoch_metrics = []\n\ndef calculate_specificity(y_true, y_pred):\n    cm = confusion_matrix(y_true, y_pred)\n    specificities = []\n    for i in range(len(cm)):\n        tn = np.sum(cm) - np.sum(cm[i,:]) - np.sum(cm[:,i]) + cm[i,i]\n        fp = np.sum(cm[:,i]) - cm[i,i]\n        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n        specificities.append(specificity)\n    return specificities\n\ndef evaluate_model(results_df, class_names):\n    label_to_idx = {label: idx for idx, label in enumerate(class_names)}\n    y_true = np.array([label_to_idx[label] for label in results_df['true_label']])\n    y_pred = np.array([label_to_idx[label] for label in results_df['predicted_class']])\n    y_scores = results_df[class_names].values\n    \n    metrics = {}\n    \n    # Balanced Accuracy\n    metrics['balanced_accuracy'] = balanced_accuracy_score(y_true, y_pred)\n    \n    # Per-class metrics\n    metrics['per_class'] = {}\n    for i, class_name in enumerate(class_names):\n        y_true_binary = (y_true == i).astype(int)\n        y_pred_binary = (y_pred == i).astype(int)\n        \n        # AUC-ROC\n        auc = roc_auc_score(y_true_binary, y_scores[:, i])\n        \n        # Calculate ROC curve for plotting\n        fpr, tpr, _ = roc_curve(y_true_binary, y_scores[:, i])\n        \n        # Precision-Recall curve\n        precision, recall, _ = precision_recall_curve(y_true_binary, y_scores[:, i])\n        ap = average_precision_score(y_true_binary, y_scores[:, i])\n        \n        # F1 Score\n        f1 = f1_score(y_true_binary, y_pred_binary)\n        \n        metrics['per_class'][class_name] = {\n            'auc': auc,\n            'roc_curve': (fpr, tpr),\n            'precision_recall': (precision, recall),\n            'average_precision': ap,\n            'f1_score': f1\n        }\n    \n    # Calculate mean metrics\n    metrics['mean_auc'] = np.mean([m['auc'] for m in metrics['per_class'].values()])\n    metrics['mean_ap'] = np.mean([m['average_precision'] for m in metrics['per_class'].values()])\n    metrics['mean_f1'] = np.mean([m['f1_score'] for m in metrics['per_class'].values()])\n    \n    # Confusion Matrix\n    metrics['confusion_matrix'] = confusion_matrix(y_true, y_pred)\n    \n    return metrics\n\ndef plot_training_history(metrics_tracker):\n    plt.figure(figsize=(12, 5))\n    \n    # Plot training curves\n    plt.subplot(1, 2, 1)\n    plt.plot(metrics_tracker.train_losses, label='Train Loss')\n    plt.plot(metrics_tracker.val_losses, label='Val Loss')\n    plt.title('Loss vs Epoch')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(metrics_tracker.train_accs, label='Train Accuracy')\n    plt.plot(metrics_tracker.val_accs, label='Val Accuracy')\n    plt.title('Accuracy vs Epoch')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy (%)')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.savefig('training_history.png')\n    plt.close()\n\ndef plot_evaluation_metrics(metrics, class_names, dataset_name):\n    # 1. ROC curves for each class\n    plt.figure(figsize=(10, 8))\n    for class_name in class_names:\n        fpr, tpr = metrics['per_class'][class_name]['roc_curve']\n        auc = metrics['per_class'][class_name]['auc']\n        plt.plot(fpr, tpr, label=f'{class_name} (AUC = {auc:.2f})')\n    \n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title(f'ROC Curves - {dataset_name}')\n    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n    plt.tight_layout()\n    plt.savefig(f'roc_curves_{dataset_name}.png')\n    plt.close()\n    \n    # 2. Precision-Recall curves\n    plt.figure(figsize=(10, 8))\n    for class_name in class_names:\n        precision, recall = metrics['per_class'][class_name]['precision_recall']\n        ap = metrics['per_class'][class_name]['average_precision']\n        plt.plot(recall, precision, label=f'{class_name} (AP = {ap:.2f})')\n    \n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title(f'Precision-Recall Curves - {dataset_name}')\n    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n    plt.tight_layout()\n    plt.savefig(f'precision_recall_{dataset_name}.png')\n    plt.close()\n    \n    # 3. Confusion Matrix\n    plt.figure(figsize=(12, 10))\n    sns.heatmap(metrics['confusion_matrix'], annot=True, fmt='d', cmap='Blues',\n                xticklabels=class_names, yticklabels=class_names)\n    plt.title(f'Confusion Matrix - {dataset_name}')\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.xticks(rotation=45, ha='right')\n    plt.yticks(rotation=0)\n    plt.tight_layout()\n    plt.savefig(f'confusion_matrix_{dataset_name}.png')\n    plt.close()\n    \n    # 4. Per-class metrics comparison\n    metrics_to_plot = ['auc', 'average_precision', 'f1_score']\n    x = np.arange(len(class_names))\n    width = 0.25\n    \n    plt.figure(figsize=(15, 8))\n    for i, metric in enumerate(metrics_to_plot):\n        values = [metrics['per_class'][c][metric] for c in class_names]\n        plt.bar(x + i*width, values, width, label=metric.replace('_', ' ').title())\n    \n    plt.xlabel('Classes')\n    plt.ylabel('Score')\n    plt.title(f'Performance Metrics by Class - {dataset_name}')\n    plt.xticks(x + width, class_names, rotation=45, ha='right')\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(f'per_class_metrics_{dataset_name}.png')\n    plt.close()\n\ndef print_metrics_summary(metrics, dataset_name):\n    print(f\"\\n=== {dataset_name} Evaluation Metrics ===\")\n    print(f\"\\nOverall Metrics:\")\n    print(f\"Balanced Accuracy: {metrics['balanced_accuracy']:.4f}\")\n    print(f\"Mean AUC: {metrics['mean_auc']:.4f}\")\n    print(f\"Mean Average Precision: {metrics['mean_ap']:.4f}\")\n    print(f\"Mean F1 Score: {metrics['mean_f1']:.4f}\")\n    \n    print(\"\\nPer-class metrics:\")\n    for class_name, class_metrics in metrics['per_class'].items():\n        print(f\"\\n{class_name}:\")\n        print(f\"  AUC-ROC: {class_metrics['auc']:.4f}\")\n        print(f\"  Average Precision: {class_metrics['average_precision']:.4f}\")\n        print(f\"  F1 Score: {class_metrics['f1_score']:.4f}\")\n\n\n\n\n\n\n\n\ndef train_epoch(model, train_loader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    for images, labels, _, _ in tqdm(train_loader, desc=\"Training\"):  # Unpack the extra dataset name\n        images, labels = images.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n    \n    epoch_loss = running_loss / len(train_loader)\n    epoch_acc = 100. * correct / total\n    \n    return epoch_loss, epoch_acc\n\n\ndef validate(model, val_loader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for images, labels, _, _ in tqdm(val_loader, desc=\"Validating\"):  # Unpack the extra dataset name\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            \n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n    \n    val_loss = running_loss / len(val_loader)\n    val_acc = 100. * correct / total\n    \n    return val_loss, val_acc\n\n\ndef evaluate_and_save(model, dataloader, output_file, class_names, is_test_set=False):\n    model.eval()\n    device = next(model.parameters()).device\n    \n    results = {\n        'image_path': [],\n        'Dataset': [],\n        'predicted_class': []\n    }\n    \n    if not is_test_set:\n        results['true_label'] = []\n        \n    for class_name in class_names:\n        results[class_name] = []\n    \n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n            if is_test_set:\n                images, img_paths, dataset_names = batch\n                images = images.to(device)\n            else:\n                images, labels, img_paths, dataset_names = batch\n                images = images.to(device)\n                results['true_label'].extend([class_names[label.item()] for label in labels])\n            \n            outputs = model(images)\n            probabilities = F.softmax(outputs, dim=1)\n            \n            results['image_path'].extend([img_path for img_path in img_paths])\n            results['Dataset'].extend([dataset_name for dataset_name in dataset_names])\n            \n            probs_np = probabilities.cpu().numpy()\n            pred_classes = torch.argmax(outputs, dim=1).cpu().numpy()\n            results['predicted_class'].extend([class_names[idx] for idx in pred_classes])\n            \n            for i, class_name in enumerate(class_names):\n                results[class_name].extend(probs_np[:, i].tolist())\n    \n    df = pd.DataFrame(results)\n    \n    if is_test_set:\n        columns = ['image_path', 'Dataset'] + class_names + ['predicted_class']\n    else:\n        columns = ['image_path', 'Dataset', 'true_label'] + class_names + ['predicted_class']\n    \n    df = df[columns]\n    df.to_excel(output_file, index=False, float_format='%.8f')\n    print(f\"Results saved to {output_file}\")\n    \n    if not is_test_set:\n        accuracy = (df['true_label'] == df['predicted_class']).mean() * 100\n        print(f\"Final Accuracy: {accuracy:.2f}%\")\n        \n    return df\n\n# Rest of the code remains unchanged...\n\ndef test_evaluate_and_save(model, dataloader, output_filename, class_names, is_test_set=False):\n    model.eval()\n    predictions = []\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n            if is_test_set:\n                images, img_paths = batch  # Unpack image and path for the test set\n                images = images.to(device)\n                outputs = model(images)\n                probs = torch.softmax(outputs, dim=1)  # Predicted probabilities\n                _, predicted = outputs.max(1)\n\n                # Collect predictions with paths and class probabilities\n                for img_path, prob, pred in zip(img_paths, probs, predicted):\n                    predictions.append({\n                        \"image_path\": img_path,\n                        **{f\"class_{i}_probability\": p.item() for i, p in enumerate(prob)},\n                        \"predicted_class\": class_names[pred.item()]\n                    })\n            else:\n                # For train/val: expect images and labels\n                images, labels = batch\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                probs = torch.softmax(outputs, dim=1)\n                _, predicted = outputs.max(1)\n\n                # Collect predictions with ground truth for train/val\n                for label, prob, pred in zip(labels, probs, predicted):\n                    predictions.append({\n                        \"true_label\": class_names[label.item()],\n                        **{f\"class_{i}_probability\": p.item() for i, p in enumerate(prob)},\n                        \"predicted_class\": class_names[pred.item()]\n                    })\n\n    # Convert predictions to DataFrame\n    df = pd.DataFrame(predictions)\n    df.to_excel(output_filename, index=False)\n    print(f\"Results saved to {output_filename}\")\n    return df\n\n\ndef main():\n    # Set device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    # Initialize model\n    model_name = \"microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\"\n    num_classes = 10\n    model = BiomedCLIPClassifier(model_name, num_classes)\n    model = model.to(device)\n    \n    # Training parameters\n    num_epochs = 3\n    batch_size = 32\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n    \n    # Initialize metrics tracker\n    metrics_tracker = MetricsTracker()\n    \n    # Create datasets and dataloaders\n    train_dataset = CustomImageDataset(root_dir='New_Dateset/Dataset/training', transform=model.preprocess)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n    \n    val_dataset = CustomImageDataset(root_dir='New_Dateset/Dataset/validation', transform=model.preprocess)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n    \n    # Training phase\n    print(\"Starting training...\")\n    best_val_acc = 0\n    \n    for epoch in range(num_epochs):\n        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n        \n        # Train\n        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n        metrics_tracker.train_losses.append(train_loss)\n        metrics_tracker.train_accs.append(train_acc)\n        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n        \n        # Validate\n        val_loss, val_acc = validate(model, val_loader, criterion, device)\n        metrics_tracker.val_losses.append(val_loss)\n        metrics_tracker.val_accs.append(val_acc)\n        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n        \n        scheduler.step()\n        \n        # Save best model\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save(model.state_dict(), 'best_model.pth')\n            print(f\"New best model saved with validation accuracy: {val_acc:.2f}%\")\n    \n    # Plot training history\n    plot_training_history(metrics_tracker)\n    \n    # Load best model for evaluation\n    print(\"\\nLoading best model for evaluation...\")\n    model.load_state_dict(torch.load('best_model.pth'))\n    model.eval()\n    \n    class_names = ['Angioectasia', 'Bleeding', 'Erosion', 'Erythema', \n                   'Foreign Body', 'Lymphangiectasia', 'Normal', 'Polyp', \n                   'Ulcer', 'Worms']\n    \n    try:\n        # Training set evaluation\n        print(\"\\nEvaluating training set...\")\n        train_df = evaluate_and_save(model, train_loader, 'efinaltrain_results.xlsx', class_names)\n        train_metrics = evaluate_model(train_df, class_names)\n        print_metrics_summary(train_metrics, \"Training Set\")\n        plot_evaluation_metrics(train_metrics, class_names, 'training')\n        \n        # Validation set evaluation\n        print(\"\\nEvaluating validation set...\")\n        val_df = evaluate_and_save(model, val_loader, 'efinalval_results.xlsx', class_names)\n        val_metrics = evaluate_model(val_df, class_names)\n        print_metrics_summary(val_metrics, \"Validation Set\")\n        plot_evaluation_metrics(val_metrics, class_names, 'validation')\n        \n        # Test set evaluation\n        print(\"\\nEvaluating test set...\")\n        test_dataset = TestDataset(root_dir='Testing set/Testing set/Images', transform=model.preprocess)\n        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n        test_df = test_evaluate_and_save(model, test_loader, 'efinaltest_results.xlsx', class_names, is_test_set=True)\n        \n        print(\"\\nAll evaluations completed successfully!\")\n        \n    except Exception as e:\n        print(f\"An error occurred during evaluation: {e}\")\n        raise\n\nif __name__ == \"__main__\":\n    main()","metadata":{},"execution_count":3,"outputs":[{"name":"stderr","output_type":"stream","text":"/raid/ai23mtech14007/.local/lib/python3.8/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n\n  warnings.warn(\n\n/raid/ai23mtech14007/.local/lib/python3.8/site-packages/open_clip/factory.py:129: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  checkpoint = torch.load(checkpoint_path, map_location=map_location)\n"},{"name":"stdout","output_type":"stream","text":"Starting training...\n\n\n\nEpoch 1/3\n"},{"name":"stderr","output_type":"stream","text":"Training: 100%|██████████| 1176/1176 [06:29<00:00,  3.02it/s]\n"},{"name":"stdout","output_type":"stream","text":"Train Loss: 0.3927 | Train Acc: 87.67%\n"},{"name":"stderr","output_type":"stream","text":"Validating: 100%|██████████| 505/505 [00:53<00:00,  9.38it/s]\n"},{"name":"stdout","output_type":"stream","text":"Val Loss: 0.2621 | Val Acc: 91.46%\n\nNew best model saved with validation accuracy: 91.46%\n\n\n\nEpoch 2/3\n"},{"name":"stderr","output_type":"stream","text":"Training: 100%|██████████| 1176/1176 [06:29<00:00,  3.02it/s]\n"},{"name":"stdout","output_type":"stream","text":"Train Loss: 0.2061 | Train Acc: 93.26%\n"},{"name":"stderr","output_type":"stream","text":"Validating: 100%|██████████| 505/505 [00:53<00:00,  9.36it/s]\n"},{"name":"stdout","output_type":"stream","text":"Val Loss: 0.2369 | Val Acc: 92.36%\n\nNew best model saved with validation accuracy: 92.36%\n\n\n\nEpoch 3/3\n"},{"name":"stderr","output_type":"stream","text":"Training: 100%|██████████| 1176/1176 [06:29<00:00,  3.02it/s]\n"},{"name":"stdout","output_type":"stream","text":"Train Loss: 0.1108 | Train Acc: 96.17%\n"},{"name":"stderr","output_type":"stream","text":"Validating: 100%|██████████| 505/505 [00:53<00:00,  9.38it/s]\n"},{"name":"stdout","output_type":"stream","text":"Val Loss: 0.1822 | Val Acc: 94.04%\n\nNew best model saved with validation accuracy: 94.04%\n\n\n\nLoading best model for evaluation...\n"},{"name":"stderr","output_type":"stream","text":"/tmp/ipykernel_1385949/2939145756.py:505: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  model.load_state_dict(torch.load('best_model.pth'))\n"},{"name":"stdout","output_type":"stream","text":"\n\nEvaluating training set...\n"},{"name":"stderr","output_type":"stream","text":"Evaluating: 100%|██████████| 1176/1176 [02:05<00:00,  9.40it/s]\n"},{"name":"stdout","output_type":"stream","text":"Results saved to efinaltrain_results.xlsx\n\nFinal Accuracy: 97.75%\n\n\n\n=== Training Set Evaluation Metrics ===\n\n\n\nOverall Metrics:\n\nBalanced Accuracy: 0.9388\n\nMean AUC: 0.9990\n\nMean Average Precision: 0.9774\n\nMean F1 Score: 0.9389\n\n\n\nPer-class metrics:\n\n\n\nAngioectasia:\n\n  AUC-ROC: 0.9984\n\n  Average Precision: 0.9732\n\n  F1 Score: 0.9298\n\n\n\nBleeding:\n\n  AUC-ROC: 0.9997\n\n  Average Precision: 0.9870\n\n  F1 Score: 0.9298\n\n\n\nErosion:\n\n  AUC-ROC: 0.9978\n\n  Average Precision: 0.9717\n\n  F1 Score: 0.9115\n\n\n\nErythema:\n\n  AUC-ROC: 0.9972\n\n  Average Precision: 0.8956\n\n  F1 Score: 0.8101\n\n\n\nForeign Body:\n\n  AUC-ROC: 0.9998\n\n  Average Precision: 0.9919\n\n  F1 Score: 0.9499\n\n\n\nLymphangiectasia:\n\n  AUC-ROC: 0.9992\n\n  Average Precision: 0.9878\n\n  F1 Score: 0.9637\n\n\n\nNormal:\n\n  AUC-ROC: 0.9996\n\n  Average Precision: 0.9999\n\n  F1 Score: 0.9948\n\n\n\nPolyp:\n\n  AUC-ROC: 0.9986\n\n  Average Precision: 0.9669\n\n  F1 Score: 0.8998\n\n\n\nUlcer:\n\n  AUC-ROC: 1.0000\n\n  Average Precision: 1.0000\n\n  F1 Score: 0.9992\n\n\n\nWorms:\n\n  AUC-ROC: 1.0000\n\n  Average Precision: 1.0000\n\n  F1 Score: 1.0000\n\n\n\nEvaluating validation set...\n"},{"name":"stderr","output_type":"stream","text":"Evaluating: 100%|██████████| 505/505 [00:53<00:00,  9.41it/s]\n"},{"name":"stdout","output_type":"stream","text":"Results saved to efinalval_results.xlsx\n\nFinal Accuracy: 94.06%\n\n\n\n=== Validation Set Evaluation Metrics ===\n\n\n\nOverall Metrics:\n\nBalanced Accuracy: 0.8464\n\nMean AUC: 0.9940\n\nMean Average Precision: 0.9093\n\nMean F1 Score: 0.8539\n\n\n\nPer-class metrics:\n\n\n\nAngioectasia:\n\n  AUC-ROC: 0.9919\n\n  Average Precision: 0.9074\n\n  F1 Score: 0.8390\n\n\n\nBleeding:\n\n  AUC-ROC: 0.9963\n\n  Average Precision: 0.9352\n\n  F1 Score: 0.8717\n\n\n\nErosion:\n\n  AUC-ROC: 0.9865\n\n  Average Precision: 0.8681\n\n  F1 Score: 0.7714\n\n\n\nErythema:\n\n  AUC-ROC: 0.9896\n\n  Average Precision: 0.7125\n\n  F1 Score: 0.6299\n\n\n\nForeign Body:\n\n  AUC-ROC: 0.9971\n\n  Average Precision: 0.9572\n\n  F1 Score: 0.8886\n\n\n\nLymphangiectasia:\n\n  AUC-ROC: 0.9951\n\n  Average Precision: 0.9476\n\n  F1 Score: 0.8919\n\n\n\nNormal:\n\n  AUC-ROC: 0.9960\n\n  Average Precision: 0.9988\n\n  F1 Score: 0.9809\n\n\n\nPolyp:\n\n  AUC-ROC: 0.9874\n\n  Average Precision: 0.7706\n\n  F1 Score: 0.7018\n\n\n\nUlcer:\n\n  AUC-ROC: 0.9999\n\n  Average Precision: 0.9972\n\n  F1 Score: 0.9791\n\n\n\nWorms:\n\n  AUC-ROC: 1.0000\n\n  Average Precision: 0.9986\n\n  F1 Score: 0.9851\n\n\n\nEvaluating test set...\n"},{"name":"stderr","output_type":"stream","text":"Evaluating: 100%|██████████| 138/138 [00:15<00:00,  8.92it/s]\n"},{"name":"stdout","output_type":"stream","text":"Results saved to efinaltest_results.xlsx\n\n\n\nAll evaluations completed successfully!\n"}]}]}